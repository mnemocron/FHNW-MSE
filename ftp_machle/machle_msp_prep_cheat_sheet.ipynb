{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb89e69d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left\"> <b> Machine Learning </b> <br> MSE FTP MachLe <br> \n",
    "<a href=\"mailto:simon.burkhardt@fhnw.ch\"> Simon Burkhardt </a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9efae1e",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "FS2021 - Examp Preparaion Notes / Cheat Sheet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99416685",
   "metadata": {},
   "source": [
    "#### How to use this sheet?\n",
    "\n",
    "> This is a collection of the most important Machine Learning facts gathered in a form to quickly find information during an exam.\n",
    "> Use `CTRL+F` during the exam to search for the topic that you are working on or to answer an question.\n",
    "\n",
    "> for general questions answer with:\n",
    ">- \"It depends ... \"\n",
    ">- \"In the case of ... I would do ...\"\n",
    "\n",
    "\n",
    "---\n",
    "this sheet is WIP\n",
    "\n",
    "**current todos**\n",
    "\n",
    "\n",
    "Bayesian Belief Networks\n",
    "\n",
    "Conditional Independence\n",
    "\n",
    "Pros. & Cons. of Hyperparameter Tuning\n",
    "\n",
    "Variance = Model Variations from training on different subsets of the data\n",
    "related to data variance\n",
    "\n",
    "Noise = Irreducible Error\n",
    "\n",
    "What are effects of Noise? --> overfitting and...?\n",
    "\n",
    "Marginalization\n",
    "\n",
    "a posteriory (MAP)\n",
    "\n",
    "What is a likelihood function?\n",
    "\n",
    "max likelihood\n",
    "\n",
    "Which model is more sensitive to outliers?\n",
    "\n",
    "Which models can generate data from learned data?\n",
    "\n",
    "Which models can complete missing values in a dataset?\n",
    "\n",
    "What is recall?\n",
    "\n",
    "Recall vs. Precision\n",
    "- Depends on what you optimize for.\n",
    "- Optimization Goal should be a single number.\n",
    "\n",
    "What is the f1-score?\n",
    "- combines Precision and Recall into a single number\n",
    "\n",
    "Area under Curve (AUC)\n",
    "\n",
    "Average Precision\n",
    "\n",
    "Regularization for Regression & regularization parameter Œª\n",
    "\n",
    "https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Mahalanobis_distance\n",
    "\n",
    "\n",
    "\n",
    "What are the different subsets of the available data splits used for?\n",
    "\n",
    "- **Training Set** to train the parameters of the **model** itself\n",
    "- **Validation Set** used to **tune the hyperparameters** of the model\n",
    "- **Test Set** to estimate the true error\n",
    "\n",
    "\n",
    "What are different methods to apply parameter tuning?\n",
    "\n",
    "- Grid-Search\n",
    "- Random-Search\n",
    "- Optimization\n",
    "    + Gradient Descent\n",
    "\n",
    "Which are linear models?\n",
    "\n",
    "Types of Norms / Metrics (L1, L2, Lp)\n",
    "\n",
    "- L1 Norm = Sparsity Inducing\n",
    "- L2 Norm = Weight Sharing\n",
    "\n",
    "### Regularization\n",
    "\n",
    "Parametrically controlling model complexity by introducing a penalty. Penalty is weighted with the regularization parameter $\\lambda$\n",
    "\n",
    "To avoid overfitting.\n",
    "\n",
    "Linear Models -- usually implemented in forms of constraints on the weights\n",
    "- Ridge regression (L2 regularization)\n",
    "- LASSO regression (L1 regularization) \n",
    "\n",
    "Support Vector Machines (SVM)\n",
    "- Parameter ‚ÄúC‚Äù controls the margin\n",
    "\n",
    "#### Exam Prompts\n",
    "\n",
    "How can this learner be improved?\n",
    "\n",
    "Would {...} help to improve this scenario?\n",
    "\n",
    "- dimmensionality reduction\n",
    "- feature transformation\n",
    "- etc...\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fe6f3d",
   "metadata": {},
   "source": [
    "### What Problems are solved by it\n",
    "\n",
    "- **Predictive Modelling** to estimate a new observation --> **regression**\n",
    "- **Explanatory Modelling** to describe the outcome when an input changes --> **classifiers**\n",
    "- **Optimizaion** to find the most relevant inputs for an optimal performance --> **dimmenstionality reduction**\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- **Supervised** Learning if you have labeled data available\n",
    "- **Unsupervised** Learning if no labels are available but you want to find patterns / clusters / structure\n",
    "- **Reinforcement** Learning to learn policies\n",
    "\n",
    "\n",
    "<img src=\"mindmaps/machine_learning_mind_map_6.png\" width=\"100%\"/>\n",
    "\n",
    "- note that **decision tree** and **random forrest** classifiers belong to the **ensemble methods**\n",
    "- note that **SVC** can also be used in **regression**\n",
    "\n",
    "### Performance Measurement Techniques\n",
    "\n",
    "What should be optimized?\n",
    "--> generally speaking: some score on the test set (not the training set)\n",
    "\n",
    "| name | calc |\n",
    "|:-----|:-----|\n",
    "| Accuracy | $$\\frac{correct}{all}=(1-error)$$ |\n",
    "| Error | $$\\frac{wrong}{all}=(1-accuracy)$$ |\n",
    "| Precision | $$\\frac{TP}{TP+FP}$$ |\n",
    "| Recall / Sensitivity | $$\\frac{TP}{TP+FN}$$ |\n",
    "| Specifity | $$\\frac{TN}{TN+FP}$$ |\n",
    "| F1 Score | $$2\\frac{Precision \\cdot Recall}{Precision + Recall}$$ |\n",
    "\n",
    "\n",
    "<img src=\"images/error_curve.png\" width=\"50%\"/>\n",
    "\n",
    "#### Recall Precision Curve (RPC)\n",
    "\n",
    "\n",
    "\n",
    "#### Recall Operator Curve (ROC)\n",
    "\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "\n",
    "### Comparison\n",
    "\n",
    "<img src=\"images/5-Table1-1.png\" width=\"100%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aecee25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Basics\n",
    "\n",
    "### Explain in your own words what _overfitting_ and _underfitting_ is\n",
    "\n",
    "(guaranteed exam question)\n",
    "\n",
    "**Overfitting** is when the model is too detailed and only learns the data by heart instead of generalizing and simplifying the data into a usable model.\n",
    "\n",
    "**Underfitting** is when the model is too simple and not able to fit important variations in the data.\n",
    "\n",
    "### Explain \"_the curse of dimensionality_\" (with an example)\n",
    "\n",
    "The more dimmenstions a dataset has, the more data is required to draw relevant conclusions from this dataset. \n",
    "The amount of data needed to support the result often grows exponentially with the dimensionality.\n",
    "\n",
    "**Example**: Say, you dropped a coin on a 100-meter line. How do you find it? Simple, just walk on the line and search. But what if it‚Äôs 100 x 100 sq. m. field? It‚Äôs already getting tough, trying to search a (roughly) football ground for a single coin. But what if it‚Äôs 100 x 100 x 100 cu.m space?! \n",
    "\n",
    "Curse of dimensionality experiment using **Norm**. \n",
    "Calculate the distance between two random points in a n-dimensional space.\n",
    "With low dimensions, the variance is high. With high dimensions the variance in the distance is low --> all arbitrary points are on average equally spaced.\n",
    "\n",
    "\n",
    "### What is the \"_no free lunch theorem_\"?\n",
    "\n",
    "For good generalization performance, there is no context-independent or usage-independent reason to favor one learning method over another.\n",
    "\n",
    "The theorem states that all optimization algorithms perform equally well when their performance is averaged across all possible problems.\n",
    "\n",
    "e.g. You cannot say things like _\"You must use KNN for your task because it performed best on my previous project\"_\n",
    "\n",
    "### What is \"_Occam's Razor_\"?\n",
    "\n",
    "When presented with competing hypotheses that make the same predictions, one should select the solution with the fewest assumptions.\n",
    "\n",
    "\n",
    "### Explain the \"_Bias Variance Dilemma_\"\n",
    "\n",
    "- **Bias** is an error caused by false assumptions and causes the algorithm to miss important features --> leads to **underfitting**\n",
    "- **Variance** is an error from sensitivity to fluctuations in the training data --> high variance leads to learning the noise and therefore **overfitting**\n",
    "\n",
    "Adding more basis functions in a linear model . . .\n",
    "\n",
    "| are these statements `true` or `false` ? | `true` | `false` |\n",
    "|:------|:--:|:--:|\n",
    "| . . . decreases model bias. | ‚úÖ |  |\n",
    "| . . . decreases estimation bias.  |  | ‚ùå |\n",
    "| . . . decreases variance.  |  | ‚ùå |\n",
    "| . . . doesn‚Äôt aÔ¨Äect bias and variance.  |  | ‚ùå |\n",
    "\n",
    "\n",
    "### Can a training Error = 0 always be achieved?\n",
    "\n",
    "(general question)\n",
    "\n",
    "(**for which algorithm is this true?**) **No**. The model is learning a function (every input maps to a unique output). \n",
    "So, two different outputs for the very same feature value is not possible!\n",
    "\n",
    "In other words, it depends on the model. If the model is too simple there will be underfitting with a lot of error.\n",
    "\n",
    "### Given m i.i.d. data points, the training error converges to the true error as m ‚Üí ‚àû.\n",
    "\n",
    "This is true, if we assume that the data points are i.i.d. (independent and identically distributed).\n",
    "A few students pointed out that this might not be the case.\n",
    "\n",
    "### Thinking about unsupervised learning, which ones of the following statements are true (multiple choice) ?\n",
    "\n",
    "| are these statements `true` or `false` ? | `true` | `false` |\n",
    "|:------|:--:|:--:|\n",
    "| DiÔ¨Äerently from partitional graph based algorithms and density estimation algorithms the K-means and the EM algorithm need the number of clusters as an input parameter. | ‚úÖ |  |\n",
    "| With hierarchical clustering algorithms based on graph theory we obtain a partition of a dataset in K diÔ¨Äerent classes. (minimal spanning tree) | ‚úÖ |  |\n",
    "| The K-Means algorithm obtains a global optimal solution for the partition of a dataset by minimizing the square distance between examples and their nearest centroid. |  | ‚ùå |\n",
    "| The EM algorithm assumes that the model of the data comes from a mixture of K n-dimensional probability distributions. |  | ‚ùå |\n",
    "\n",
    "K-means is sensitive to the seed centroids --> not the optimal solution\n",
    "\n",
    "### How do you ensure you‚Äôre not overÔ¨Åtting with a model ?\n",
    "\n",
    "| are these statements `true` or `false` ? | `true` | `false` |\n",
    "|:------|:--:|:--:|\n",
    "| Keep the model simpler: reduce variance by taking into account fewer variables and parameters, thereby removing some of the noise in the training data. | ‚úÖ |  |\n",
    "| Use cross-validation techniques such as k-fold cross-validation. |  | ‚ùå |\n",
    "| Use regularization techniques such as LASSO that penalize certain model parameters if they‚Äôre likely to cause overÔ¨Åtting. | ‚úÖ |  |\n",
    "| Generating an ensemble of learners and using a softmax or voting output. | ‚úÖ |  |\n",
    "\n",
    "Cross-Validation does not change the learner (!)\n",
    "\n",
    "### What causes Bias?\n",
    "\n",
    "- too simple assumptions / underfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8bc4e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data Management\n",
    "\n",
    "Business Understanding -- Question/ Problem Formulation\n",
    "- What problems are we trying to solve?\n",
    "- What is our hypothesis (prior knowledge)?\n",
    "- What is our metric of success?\n",
    "\n",
    "Data Understanding\n",
    "- Do we already have relevant data?\n",
    "- Where are the biases, anomalies or other issues with the data?\n",
    "- How to transform the data to enable effective analysis?\n",
    "\n",
    "Data Preparation\n",
    "- What data do we have and what data do we need?\n",
    "- How will we collect more data?\n",
    "- How will we organize the data?\n",
    "\n",
    "Modeling and Evaluation\n",
    "- What does the data say about the world?\n",
    "- Does it actually answer the question we are looking for?\n",
    "- How robust are the conclusions?\n",
    "\n",
    "> \"Good data preparation includes **cleaning**, **transforming** and **aggregating** model \n",
    "inputs as well as the identification and treatment of **outliers**.\"\n",
    "\n",
    "Data Cleaning\n",
    "- Fix input errors, typos, remove duplicates, etc.\n",
    "\n",
    "Imputation\n",
    "- Fix missing values\n",
    "\n",
    "Transformations\n",
    "- Binarization, One-hot-encoding\n",
    "- Binning -- discretize numeric values\n",
    "- BoxCox/ Power Transformation\n",
    "\n",
    "Scaling and Normalization\n",
    "- whiting, c.f. PCA, gradient descent methods\n",
    "\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "Feature Design\n",
    "- statistical values of ‚Äúraw data‚Äù, e.g. mean, median, var, etc.\n",
    "- Domain knowledge/ invariances!\n",
    "- Simplification, reduce redundancies\n",
    "\n",
    "Feature Combination\n",
    "- e.g., 3D motion vector from x,y,z gyroscope\n",
    "\n",
    "Feature expansion\n",
    "- c.f. SVM (kernel trick) \n",
    "\n",
    "Feature Selection\n",
    "- \"What are relevant features for predicting the task at hand?\"\n",
    "- Reducing the number of Features in order to\n",
    "    + reduce overfitting and hence improve generalization -- **decrease dimensions**;\n",
    "    + to gain a better understanding of relevant features and their influence on the output -- **explainability & interpretability**\n",
    "\n",
    "Univariate Feature Selection\n",
    "- correlation between a feature and the target variable\n",
    "\n",
    "Regularization\n",
    "- e.g., linear model + Lasso, Ridgid \n",
    "\n",
    "Subset Selection\n",
    "- Feed forward Feature Selection\n",
    "- Backwards Feature Selection (Recursive Feature Elimination)\n",
    "- combinations\n",
    "\n",
    "Model based Feature Ranking\n",
    "‚Ä¢ e.g., random forests (cf. last lecture)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc86d7a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Classifiers\n",
    "\n",
    "### A classiÔ¨Åer trained on less training data is less likely to overÔ¨Åt?\n",
    "\n",
    "This is false. A speciÔ¨Åc classiÔ¨Åer (with some Ô¨Åxed model complexity) will be more likely\n",
    "to overÔ¨Åt to noise in the training data when there is less training data, and is therefore\n",
    "more likely to overÔ¨Åt.\n",
    "\n",
    "### What is the difference between a classifier and an estimator?\n",
    "\n",
    "**estimator**: This isn't a word with a rigorous definition but it usually associated with finding a current value in data. it is most frequently used in conjunction with **parameter estimation** or **density estimation**.\n",
    "The estimator then would be the method of generating estimations, for example the method of **maximum likelihood**.\n",
    "\n",
    "**classifier**: This specifically refers to a type of function where the response is **discrete**. Compared to this a regressor will have a continuous response. \n",
    "\n",
    "### What is the difference between a generative model and a discriminative model?\n",
    "\n",
    "- A **Generative** Model explicitly models the actual distribution of each class.\n",
    "- A **Discriminative** model models the decision boundary between the classes. \n",
    "\n",
    "**Generative classifiers**\n",
    "\n",
    "- Na√Øve Bayes\n",
    "- Bayesian networks\n",
    "- Markov random fields\n",
    "- Hidden Markov Models (HMM)\n",
    "\n",
    "**Discriminative Classifiers**\n",
    "\n",
    "- Logistic regression\n",
    "- Suport Vector Machine (SVM)\n",
    "- Traditional neural networks\n",
    "- k-Nearest neighbour (KNN)\n",
    "- Conditional Random Fields (CRF)\n",
    "\n",
    "What happens when training data is biased over one class in Generative Model?\n",
    "What happens when training data is biased over one class in Discriminative Models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bde6d3",
   "metadata": {},
   "source": [
    "## KNN - k-nearest Neighbour\n",
    "\n",
    "> KNN is good for low dimmensions\n",
    "> - for many features (e.g. pixles) the data must be broken down\n",
    "\n",
    "kNN cannot extrapolate outside of the min/max of the dataset --> **limited regression**\n",
    "\n",
    "| are these statements `true` or `false` ? | `true` | `false` |\n",
    "|:------|:--:|:--:|\n",
    "| The classiÔ¨Åcation accuracy is better with larger values of k. |  | ‚ùå | \n",
    "| The decision boundary is smoother with smaller values of k.  |  | ‚ùå |\n",
    "| The decision boundary is linear. |  | ‚ùå |\n",
    "| k-NN does not require an explicit training step. | ‚úÖ |  |\n",
    "\n",
    "### How can you prevent K-means algorithm from getting stuck in bad local optima ?\n",
    "\n",
    "| are these statements `true` or `false` ? | `true` | `false` |\n",
    "|:------|:--:|:--:|\n",
    "| Set the same seed value for each run. |  | ‚ùå |\n",
    "| Use multiple random initializations.  | ‚úÖ |  |\n",
    "| Taking bootstrap samples of the data and run it several times. | ‚úÖ |  |\n",
    "| Using K-means++ | ‚úÖ |  |\n",
    "\n",
    "\n",
    "### Given a k-NN classiÔ¨Åer which ones of the following statements are true (multiple choice) \n",
    "\n",
    "| are these statements `true` or `false` ? | `true` | `false` |\n",
    "|:------|:--:|:--:|\n",
    "| The more examples are used for classifying an example, the higher is the accuracy we obtain. (no help if overlap in data) |  | ‚ùå |\n",
    "| The more attributes we use to describe the examples the more diÔ¨Äcult it is to obtain high accuracy. | ‚úÖ |  |\n",
    "| The most costly part of this method is to learn the model. |  | ‚ùå |\n",
    "| We can use k-NN for classiÔ¨Åcation and regression. | ‚úÖ |  |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab9a0e7",
   "metadata": {},
   "source": [
    "## SVM/SVC - Support Vector Machine\n",
    "\n",
    "- SVC = only a classifier\n",
    "- SVM = classifier with built in Kernel Trick\n",
    "\n",
    "**Classifier** and **Regression**\n",
    "\n",
    "SVM family of classifiers\n",
    "\n",
    "1. (Maximal margin) hyperplane classifier (for linearly separable data)\n",
    "2. Support Vector **Classifier** SVC (for almost linearly separable ÔÉ†soft margins, slack variable)\n",
    "3. Support Vector **Machine** SVM (for non linearly separable data, **Kernel Classifiers**)\n",
    "\n",
    "- SVMs do not directly provide **probability estimates**, these are calculated using an expensive five-fold cross-validation\n",
    "\n",
    "Advantages\n",
    "- SVMs are effective in high dimensional spaces\n",
    "- _\"SVM are still eÔ¨Äective in cases where number of dimensions d is greater than the number of samples N\"_\n",
    "- insensitive to distant points\n",
    "- only the closest points (support vectors) are required to describe the model (memory efficient)\n",
    "\n",
    "Disadvantages\n",
    "- sensitive to noise\n",
    "- not scale invariant, so it is highly recommended to scale your data\n",
    "- not suitable for large data sets\n",
    "- not suited for \n",
    "- the SVC does not output a probability of a classification\n",
    "\n",
    "### Explain the workings of the $C$ and $\\gamma$ parameters\n",
    "\n",
    "Parameters $C$ and $\\gamma$ (with Gaussian kernel) allow to adjust the flexibility of SVM (bias variance \n",
    "trade off) \n",
    "\n",
    "The parameter $C$ controlls the **Soft Margins** version of the SVC where the data is not linearly separable but **misclassification is penalized** with the weight $C$.\n",
    "\n",
    "- If you have a lot of noisy observations you should decrease $C$\n",
    "- **decreasing $C$** corresponds to **more regularization**, **larger margin**, **higher tolerance** for misclassification, **less confidence** in the dataset (more noise)\n",
    "\n",
    "**RBF Kernel** has the parameter $\\gamma$\n",
    "\n",
    "$$\n",
    "K(x_i,x_j)=exp( -\\gamma ||x_i - x_j||^2 )\n",
    "$$\n",
    "\n",
    "> Intuitively, the **gamma** parameter defines how far the influence of a single training example reaches, with low values meaning ‚Äòfar‚Äô and high values meaning ‚Äòclose‚Äô. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.\n",
    "\n",
    "> The behavior of the model is very sensitive to the gamma parameter. If gamma is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with C will be able to prevent overfitting.\n",
    "\n",
    "> When gamma is very small, the model is too constrained and cannot capture the complexity or ‚Äúshape‚Äù of the data. The region of influence of any selected support vector would include the whole training set. The resulting model will behave similarly to a linear model with a set of hyperplanes that separate the centers of high density of any pair of two classes.\n",
    "\n",
    "### What do you do when the data is not linearly sepparable?\n",
    "\n",
    "Increase the dimensionality by transforming the data into a higher dimensional feature space\n",
    "\n",
    "### Write down and explain the primal loss function of a SVC for a binary classification problem\n",
    "\n",
    "When the data is linearly separable by a hyperplane, many hyperplanes will fit in between them.\n",
    "The question is, which plane separates the data the best?\n",
    "There needs to be a loss function to maximize the distance to each supporting point.\n",
    "Normalization is also needed in the form of the **geometric margin**.\n",
    "\n",
    "minimax Problem: find the minimal distance, and then maximize this minimal distance.\n",
    "\n",
    "### Explain the Representer Theorem \n",
    "\n",
    "The Representer Theorem states that the solution ùë§‚àó of the optimization problem can always \n",
    "be written as a linear combination of the training data:\n",
    "\n",
    "### Why is it important to scale the inputs when using SVM?\n",
    "\n",
    "> SVMs try to Ô¨Åt the largest possible ¬´street¬ª between the classes (see the Ô¨Årst answer), so if the training set is not scaled, the SVM will tend to neglect small features.\n",
    "\n",
    "### What is the fundamental idea behind SVMs?\n",
    "\n",
    "> The fundamental idea behind Support Vector Machines is to Ô¨Åt the widest possible ¬´street¬ªbetween the classes. In other words, the goal is to have the largest possible margin between the decision boundary that separates the two classes and the training instances.\n",
    "When performing soft margin classiÔ¨Åcation, the SVM searches for a compromise between perfectly separating the two classes and having the widest possible street (i.e., a few in-stances may end up on the street). Another key idea is to use kernels when training on nonlinear datasets.\n",
    "\n",
    "### What is a support vector?\n",
    "\n",
    "> After training an SVM, a support vector is **any instance located on the ¬´street¬ª, including its border**. The decision boundary is entirely determined by the support vectors. Any instance that is not a support vector (i.e., oÔ¨Ä the street) has no inÔ¨Çuence whatsoever; you could remove them, add more instances, or move them around, and as long as they stay oÔ¨Ä the street they won‚Äôt aÔ¨Äect the decision boundary. Computing the predictions only involves the support vectors, not the whole training set.\n",
    "\n",
    "### Can an SVM output a conÔ¨Ådence score when it classiÔ¨Åes an instance? What about a probability?\n",
    "\n",
    "cnofidence: **YES** / probability: **NO**\n",
    "\n",
    "> An SVM classiÔ¨Åer can output the distance between the test instance and the decision boundary, and you can use this as a conÔ¨Ådence score. However, this score cannot be directly converted into an estimation of the class probability. If you set probability=True when creating an SVM in Scikit-Learn, then after training it will calibrate the probabilities using Logistic Regression on the SVM‚Äôs scores (trained by an additional Ô¨Åve-fold cross-validation on the training data). This will add the `predict_proba()` and `predict_log_proba()` methods to the SVM.\n",
    "\n",
    "### Should you use the primal or the dual form of the SVM problem to train a  model on a training set with millions of instances and hundreds of features?\n",
    "\n",
    "> This question applies only to linear SVMs since kernelized can only use the dual form. The computational complexity of the primal form of the SVM problem is proportional to the number of training instances N, while the computational complexity of the dual form is proportional to a number between N2 and N3. So if the3re are millions of instances, you should deÔ¨Ånitely use the primal form, because the dual form will be much too slow.\n",
    "\n",
    "### Say you trained an SVM classiÔ¨Åer with an RBF kernel. It seems to underÔ¨Åt  the training set: should you increase or decrease Œ≥? What about C?\n",
    "\n",
    "> If an SVM classiÔ¨Åer trained with an RBF kernel underÔ¨Åts the training set, there might be too much regularization. To decrease it, you need to increase $\\gamma$ or $C$ (or both).\n",
    "\n",
    "### What are Lagrange-Multipliers and why are they used in SVM?\n",
    "\n",
    "(Quadratic Optimization)\n",
    "\n",
    "To get rid of the linear inequality constraints, one usually applies Lagrange multipliers\n",
    "\n",
    "### What is a dual form of an optimizaiton problem?\n",
    "\n",
    "> At first sight the dual form appears to have the disadvantage of a K-NN classifier ‚Äî it requires \n",
    "the training data points $x_i$. However, many of the $a_i$'s are zero. The ones that are non-zero \n",
    "define the support vectors $x_i$.\n",
    "\n",
    "> Solving the primal problem, we obtain the optimal $w$, but **know nothing about the $a_i$**. In order to classify a query point $x$ we need to explicitly compute the scalar product $w^Tx$, which **may be expensive if $d$ is large**.\n",
    "Solving the dual problem, we obtain the $a_i$\n",
    "(where $a_i=0$ for all but a few points - the support vectors). In order to classify a query point $x$, we calculate\n",
    ">$$\n",
    "w^Tx+w_0= \\left( \\sum_{i=1}^{n} a_i y_i x_i \\right)^T x+w_0= \\sum_{i=1}^{n} a_i y_i ‚ü®xi,x‚ü©+w_0\n",
    "$$\n",
    ">This term is **very efficiently** calculated if there are only few support vectors. Further, since we now have a scalar product only involving data vectors, we may apply the **kernel trick**.\n",
    "\n",
    "### What does the geometric margin say?\n",
    "\n",
    "The geometric margin is a way to measure the distance betwenn the separating hyperplane and the data points.\n",
    "\n",
    "> The geometric margin is invariant to rescaling of the parameters; i.e., if we replace ùíò with \n",
    "ùüêùíò and ùíÉ with ùüêùíÉ, then the geometric margin does not change.\n",
    "\n",
    "### How do you distinguish \"linearly non-separable classes\"?\n",
    "\n",
    "1. if the classes only slightly overlap: use **Soft Margin SVC** with the correct regularization $C$\n",
    "2. if the classes overlap more: transform to higher dimensional space using the **Kernel Trick** in a **SVM**\n",
    "\n",
    "### What is the Kernel Trick and how is it useful?\n",
    "\n",
    "> The Kernel trick involves kernel functions that can enable **in higher-dimension spaces**\n",
    "**without** explicitly **calculating the coordinates of points** within that dimension: instead,\n",
    "kernel functions compute the inner products between the images of all pairs of data in a\n",
    "feature space. This allows them the very useful attribute of calculating the coordinates\n",
    "of higher dimensions while being **computationally cheaper** than the explicit calculation\n",
    "of said coordinates. Many algorithms can be expressed in terms of inner products. Using\n",
    "the kernel trick enables us eÔ¨Äectively run algorithms in a high-dimensional space with\n",
    "lower-dimensional data.\n",
    "\n",
    "### What is the complexity of the kernel trick?\n",
    "\n",
    "> The Complexity of learning depends on $N$, typically it is $O(N^3)$ , not on $D$\n",
    "\n",
    "- N dimensional vector\n",
    "- D dimensional space\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6f863",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "> However, although naive Bayes is known as a decent classiÔ¨Åer,\n",
    "it is known to be a bad estimator, so the probability outputs from `predict_proba` are not to be\n",
    "taken too seriously.\n",
    "\n",
    "### What is a weakness of Bayes Classifier and what can be done against it?\n",
    "\n",
    "- The classifer becomes weak, if a certain state was never observed\n",
    "- Use Laplace Smoothing\n",
    "\n",
    "\n",
    "### Suppose X and Y are two independent Gaussian random variables. Which of the following variable is Gaussian random variable?\n",
    "\n",
    "\n",
    "| are these statements `true` or `false` ? | `true` | `false` |\n",
    "|:------|:--:|:--:|\n",
    "| $4X+3Y$ | ‚úÖ |  |\n",
    "| $X^2$ |  | ‚ùå |\n",
    "| $X\\cdot Y$ |  | ‚ùå |\n",
    "| $$ |  | ‚ùå |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8870d246",
   "metadata": {},
   "source": [
    "## Decision Tree (classifier)\n",
    "\n",
    "\n",
    "### Early stopping to avoid overfitting and reduce complexity\n",
    "\n",
    "- max. depth of the decision tree\n",
    "- min. number of samples in a leaf node\n",
    "- train until x% of the data is explained\n",
    "- etc.\n",
    "\n",
    "### What are advantages and disadvantages of decision trees?\n",
    "\n",
    "Advantages:\n",
    "- Clear Visualization\n",
    "- Simple and easy to understand\n",
    "- for **classification** and **regression** problems\n",
    "- no feature scaling required\n",
    "- handles non-linearity efficiently\n",
    "- robust to **outliers**\n",
    "- short training preiod (vs. random forrest)\n",
    "\n",
    "Disadvantages:\n",
    "- tends to overfit if no stop condition is defined\n",
    "- high variance in the model and output because of the overfitting issue\n",
    "- instability when regenerating the tree based on new data\n",
    "- affected by **noise** \n",
    "- not suitable for large datasets\n",
    "\n",
    "\n",
    "### How can you improve ... ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f54fe",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "- _\"What is better than having an ML model - having multiple models!\"_\n",
    "- Take a weak learning algorithm... only requirement should be better than random guessing ... and turn it into a strong one by making it focus on difficult cases.\n",
    "- The error rate of each classifier must be <50% in order for the combined error to decrease.\n",
    "\n",
    "applied to **Classifiers**\n",
    "- taking the majority vote from different classifiers results\n",
    "- theory \n",
    "    + assumes that the classifiers are independent\n",
    "    + Variance reduces linearly\n",
    "    + Bias remains constant\n",
    "- practice\n",
    "    + classifiers are dependent (because of the same training data)\n",
    "    + Variance reduces sub-linearly\n",
    "    + Bias increases slightly\n",
    "\n",
    "applied to **Regression**\n",
    "- **average the results to decrease the standard deviation**\n",
    "- std.dev is reduced by a factor of $\\sqrt{n}$\n",
    "\n",
    "### What is boosting?\n",
    "\n",
    "The multiple models are trained with the data samples that fail to be classified by the previous model.\n",
    "\n",
    "- The 1. model has all of the training data available. This Model has the highest **weight**\n",
    "- the next model has only the difficult cases that are wrongly classified by the previous model to train\n",
    "\n",
    "- randomly sample with replacement over weighted data\n",
    "- **minimizes bias** (and variance) -- fights **underfitting** (and overfitting)\n",
    "\n",
    "### What is bagging (bootstrapping)?\n",
    "\n",
    "(**B**ootstrap + **agg**regation)\n",
    "\n",
    "Train the classifiers with subsets of the original data and repeat some samples (replace).\n",
    "\n",
    "- randomly sample with replacement\n",
    "- **minimizes variance** (usually cannot reduce bias) -- fights **overfitting**!\n",
    "- computational efficient (note: all $M$ models can be trained in parallel)\n",
    "\n",
    "\n",
    "### What is an out of bag error (OOB)?\n",
    "\n",
    "An OOB is an error during the validation with a validation set.\n",
    "\n",
    "From the full dataset there is the training dataset available to train the full Random Forrest Algorithm.\n",
    "The individual sub-trees use again a subsamble from the global training set and a subset for validation (out of bag).\n",
    "An error occurring for one subtree from a validation set is an OOB error.\n",
    "\n",
    "\n",
    "## Random Forrest Classifier\n",
    "\n",
    "- is an ensemble method\n",
    "- RF is not in all situations the best method, however it often simply works\n",
    "\n",
    "\n",
    "- Grow many trees on bootstrapped samples of training data\n",
    "- **Minimize bias** by growing trees sufficiently deep (overfitting)\n",
    "- **Maximize variance reduction** by minimizing correlation between trees by means of bootstrapping data for each tree and sampling variable set at each node (usually $\\sqrt{M}$).\n",
    "- Reduce variance of noisy but **unbiased** trees by averaging\n",
    "\n",
    "Advantages:\n",
    "- Simple -- no assumption of the underlying distribution\n",
    "- OOB error for free\n",
    "- Many variables, even when they are not relevant for the task at hand or noisy\n",
    "- Robust against **outliers**\n",
    "- Multiclass\n",
    "- Limit overfitting (**trees have to be independent**!)\n",
    "- Unbalanced dataset (subsampling)\n",
    "\n",
    "Disadvantages\n",
    "- requires much computational power as well as resources as it builds numerous trees to combine their outputs\n",
    "- requires much time for training as it combines a lot of decision trees to determine the class\n",
    "- due to the ensemble of decision trees, it also **suffers interpretability** \n",
    "- fails to determine the **significance of each variable**\n",
    "\n",
    "### What additional method is used by Random Forest compared to bagging trees.\n",
    "### How does Random Forest relate to decision trees and bagging.\n",
    "\n",
    "The fundamental difference is that in random forests, **only a subset of features are selected at random** out of the total and the best split feature from the subset is used to split each node in a tree, unlike in bagging where all features are considered for splitting a node.\n",
    "\n",
    "### Why are Random Forest robust against outliers?\n",
    "\n",
    "The intuitive answer is that a decision tree works on splits and splits aren't sensitive to outliers: a split only has to fall anywhere between two groups of points to split them. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef190d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Clustering (classification of unlabelled data)\n",
    "\n",
    "### What is the difference between Hard- and Soft Clustering?\n",
    "\n",
    "**Hard Clustering** \n",
    "\n",
    "**Soft Clustering**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e57bc8",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "###  What are the basic assumptions of the k-Means algorithm? How does it work? \n",
    "\n",
    "- Each data point is closer to its own cluster center than the other cluster centers\n",
    "- A cluster center is the arithmetic mean of all the points that belong to the cluster.\n",
    "\n",
    "### What are Advantages and Disadvantages of the k-means Clustering?\n",
    "\n",
    "Advantages:\n",
    "- \n",
    "\n",
    "Disadvantages:\n",
    "- K-Means is known as a hard clustering algorithm because clusters are not allowed to overlap\n",
    "- k-means cannot learn the optimal number of clusters from the data. If we ask for six clusters it will find six clusters, which may or may not be meaningful.\n",
    "    + use a more complex clustering algorithm like Gaussian Mixture Models, or one that can choose a suitable number of clusters (DBSCAN, mean-shift, affinity propagation)\n",
    "- k-means is terrible for non-linear data: this results because of the assumption that points will be closer to their own cluster center than others\n",
    "    + transform data into higher dimension where linear separation is possible e.g., spectral clustering\n",
    "- A resulting issue of K-Means' circular boundaries is that it has no way to account for oblong or elliptical clusters.\n",
    "    + 1. measure uncertainty in cluster assignments by comparing distances to all cluster centers\n",
    "    + 2. allow for flexibility in the shape of the cluster boundaries by using ellipses\n",
    "- \n",
    "\n",
    "### What is the k-means++ Algorithm?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9055fc3f",
   "metadata": {},
   "source": [
    "## EM - Expectation Maximization\n",
    "\n",
    "### Explain how the expecation maximization algorithm (EM) works.\n",
    "\n",
    "\n",
    "\n",
    "### List a Ô¨Åve data science applications where EM can be used.\n",
    "\n",
    "\n",
    "\n",
    "### How would you prove that the EM algorithm converges to the maximum likelihood estimate of the hypothesis made?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc89ae",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Dimmensionality Reduction\n",
    "\n",
    "### Thinking about dimensionality reduction and attribute selection, which ones of the following statements are true (multiple choice) ?\n",
    "\n",
    "\n",
    "| are these statements `true` or `false` ? | `true` | `false` |\n",
    "|:------|:--:|:--:|\n",
    "| Wrapper methods usually provide the best performing feature set for a particular type of model. However, they are computationally intensive since for each subset a new model needs to be trained. | ‚úÖ |  |\n",
    "| Filter methods are independent to the type of predictive model. Common measures are distance metrics, correlation, mutual information, and consistency metrics. | ‚úÖ |  |\n",
    "| Wrappers are feature selection methods that, given a classifer as a performance criteria, search in the space of subset of features (feature combinations) for the minimal one that obtains the higher accuracy. | ‚úÖ |  |\n",
    "| Filters are unsupervised feature selection methods because they use evaluation criteria from the intrinsic connections between features to score a feature subset. | ‚úÖ |  |\n",
    "\n",
    "\n",
    "### Which dimmensionality reduction algorithm do you know?\n",
    "\n",
    "- **Kernel PCA** is generally well suited in reducing the dimensionality of high dimensional, nonlinear datasets. By applying the kernel trick, a nonlinear mapping is applied to the input data, actually increasing the dimensionality even more. The kernel however can be evaluated in dataspace. The problem complexity is given by the number of data points.\n",
    "- **Local Linear Embedding (LLE)** reduces dimensionality while trying to preserve the distances between close instances only.\n",
    "- **Isomap** creates a graph by connecting each instance to its nearest neighbors, then reduces dimensionality while trying to preserve the geodesic distances between the instances.\n",
    "- **t-Distributed Stochastic Neighbor Embedding (t-SNE)** reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space (e.g., to visualize the MNIST images in 2D).\n",
    "- **Linear Discriminant Analysis (LDA)** is actually a classiÔ¨Åcation algorithm. During training it learns the most discriminative axes between the classes. These axes can be used to deÔ¨Åne a hyperplane onto which to project the data. The projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classiÔ¨Åcation algorithm such as an SVM classiÔ¨Åer.\n",
    "\n",
    "### How can you evaluate the performance of a dimensionality reduction algorithm?\n",
    "\n",
    "(general question)\n",
    "\n",
    "One way to measure the performance of the reduction alone is to apply the reverse transformation and measure the reconstruction error. This only works, if an inverse can be aplied.\n",
    "\n",
    "It depends on what performance measurement the entire machine learning task optimizes for. In order for this question to be answered, a performence metric must also be defined (e.g. Precision, Recall, f1-score etc.)\n",
    "\n",
    "\n",
    "### Does it make sense to chain two different dimensionality reduction algorithms?\n",
    "\n",
    "(general question)\n",
    "\n",
    "It depends on the task. It could help to increase the speed.\n",
    "\n",
    "> A common example is using PCA to quickly get rid of a large number of useless dimensions, then applying another much slower dimensionality reduction algorithm, such as LLE. This two-step approach will likely yield the same performance as using LLE only, but in a fraction of the time.\n",
    "\n",
    "### What are the main motivations for reducing a dataset‚Äôs dimensionality?\n",
    "\n",
    "- noise reduction, removal\n",
    "- removal of noise and redundant features for classiÔ¨Åcation tasks\n",
    "- algorithms perform better and faster with less dimensions\n",
    "- removes redundant features\n",
    "- allows to filter for features that matter and discard features that have no additional information\n",
    "- visualization of high dimensional data\n",
    "- data compression\n",
    "\n",
    "### What are the main drawbacks of dimensionality reduction techniques?\n",
    "\n",
    "- information is lost on purpose\n",
    "- linear methods are not able to unwrap / detect substructures\n",
    "- the reduction process is again a step in the machine learning chain that uses processing power and time\n",
    "- adds complexity to the leaerning pipeline\n",
    "- transformed spaces are hard to interpret by humans\n",
    "\n",
    "### Once a dataset‚Äôs dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?\n",
    "\n",
    "(general question)\n",
    "\n",
    "For some methods **yes**. The data can be decompressed but will be contaminated with some artifacts because some information was lost.\n",
    "\n",
    "Also, not all methods have an inverse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c59b0",
   "metadata": {},
   "source": [
    "## PCA - Principle Component Analysis\n",
    "\n",
    "\n",
    "### Can PCA be used to reduce the dimensionality of a highly nonlinear dataset? Which methods can alternatively be used?\n",
    "\n",
    "No, PCA/NDS acts globally.\n",
    "\n",
    "Use a Kernel instead. \n",
    "\n",
    "> If highly nonlinear distribution, which would you select ( ... ) ?\n",
    "\n",
    "###  Suppose you perform a PCA on a 1000-dimensional dataset, setting the explained variance to 95%. How many dimensions will the resulting dataset have?\n",
    "\n",
    "(general question)\n",
    "\n",
    "cannot be answered. depends on data\n",
    "\n",
    "### In which cases would you use incremental PCA, randomized PCA or kernel PCA?\n",
    "\n",
    "\n",
    "- **Randomized PCA** is useful when you want to considerably reduce dimensionality and the dataset Ô¨Åts in memory; in this case, it is much faster than regular PCA. Finally, Kernel PCA is useful for nonlinear datasets.\n",
    "- **Randomized PCA** to save computation time by selecting a randomized subset of the data. \n",
    "- **kernel PCA** is used for nonlinear datasets\n",
    "- **Regular PCA** is the default, but it works only if the dataset Ô¨Åts in memory.\n",
    "- **Incremental PCA** is useful for large datasets that don‚Äôt Ô¨Åt in memory, but it is slower than regular PCA, so if the dataset Ô¨Åts in memory you should prefer regular PCA. Incremental PCA is also useful for online tasks, when you need to apply PCA on the Ô¨Çy, every time a new instance arrives.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208ae5f1",
   "metadata": {},
   "source": [
    "## Kernel Functions\n",
    "\n",
    "\n",
    "### Explain what a kernel is.\n",
    "\n",
    "\n",
    "\n",
    "### List four commonly used kernels $k(x, x‚Ä≤)$ in Machine Learning.\n",
    "\n",
    "\n",
    "\n",
    "### Show, that the RBS-kernel is symmetric and positive semi-deÔ¨Ånite.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13a29c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Training & Validation\n",
    "\n",
    "\n",
    "### Debugging Questions\n",
    "\n",
    "Suppose you are using some classiÔ¨Åer algorithm on a given training set. The training error\n",
    "is acceptable. However, it makes unacceptably large errors in its predictions on unseen data.\n",
    "What should be tried next?\n",
    "\n",
    "| are these statements `true` or `false` ? | `true` | `false` |\n",
    "|:------|:--:|:--:|\n",
    "| Get more training samples. | ‚úÖ |  |\n",
    "| Try larger sets of features, e.g. by generating polynomial features. |  | ‚ùå |\n",
    "| Decrease the regularization parameter Œª if regularization is used.  |  | ‚ùå |\n",
    "| Create a meta learner, e.g. a voting or bagging classiÔ¨Åer out of several diÔ¨Äerent classiÔ¨Åers or build an ensemble of classiÔ¨Åers. | ‚úÖ |  |\n",
    "\n",
    "### Validation Curve\n",
    "\n",
    "| are these statements `true` or `false` ? | `true` | `false` |\n",
    "|:------|:--:|:--:|\n",
    "| A validation curve shows the training error as function of the training data size. |  | ‚ùå |\n",
    "| A validation curve shows the training error as function of the hyperparameter. | ‚úÖ |  |\n",
    "| If the learning curve shows a large gap between training and test error, the learner most probably suÔ¨Äers from a high bias problem. |  | ‚ùå |\n",
    "| If the learning curve shows a large gap between training and test error, the learner most probably suÔ¨Äers from a high variance problem. | ‚úÖ |  |\n",
    "| Hyperparameter tuning shoud be done using a cross-validated grid search within the inner loop. | ‚úÖ |  |\n",
    "|  In a Bayesian approach, the hyperparameters can be deÔ¨Åned by a prior distribution. Using Bayes theorem to calculate the posterior, we get a point estimate of the hyperparameters. |  | ‚ùå |\n",
    "\n",
    "If performance overall is bad (20%) in the learning curve, it is an indication of **high bias**\n",
    "\n",
    "### Estimation Error\n",
    "\n",
    "Thinking about the estimation error with the training set for a learning method. Which of the\n",
    "following statements are true? (multiple answer)\n",
    "\n",
    "| are these statements `true` or `false` ? | `true` | `false` |\n",
    "|:------|:--:|:--:|\n",
    "| The estimation error of the decision tree built for the training set without pruning on the trainng data is zero. | ‚úÖ |  |\n",
    "| The estimation error for the training set for the built 3-KNN classiÔ¨Åer with this data is zero. |  | ‚ùå |\n",
    "| The estimation error for the training set in the decision tree constructed with this data after the pruning procedure is zero. |  | ‚ùå |\n",
    "| The estimation error for the training set for the built 1-KNN classiÔ¨Åer with this is zero. | ‚úÖ |  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adabcba",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "1. Split Data into Training and Test data\n",
    "2. Training Data + Cross-Validation -> best Hyperparameters\n",
    "3. Retrain model with best Hyperparameters on the entire Training Data\n",
    "4. Test Data + Model -> estimate of generalization error\n",
    "\n",
    "Note: The estimate of the generalization error can be cross-validated as well\n",
    "\n",
    "Advantages:\n",
    "- make to most out of the examples!\n",
    "- averages out do individual performance of the splits (especially for few data-points)\n",
    "\n",
    "Types of Validaton:\n",
    "- 2-fold cross-validation \n",
    "- k-fold cross-validation \n",
    "- LOOCV (leave-one-out-...)\n",
    "\n",
    "### The numerical complexity of $k$-fold crossvalidation is...\n",
    "\n",
    "| are these statements `true` or `false` ? | `true` | `false` |\n",
    "|:------|:--:|:--:|\n",
    "| linear in $k$ | ‚úÖ |  |\n",
    "| quadratic in $k$ |  | ‚ùå |\n",
    "| cubic in $k$ |  | ‚ùå |\n",
    "| exponential in $k$ |  | ‚ùå |\n",
    "\n",
    "\n",
    "## Validation Curve\n",
    "\n",
    "\n",
    "\n",
    "## Hyper Parameter Tuning\n",
    "\n",
    "### How do you tune hyper parameters?\n",
    "\n",
    "- use grid search (linear or logarithmic step sizes)\n",
    "\n",
    "### What are disadvantages of grid search?\n",
    "\n",
    "- Smaller step size corresponds to more scenarios to be tested which increases the training time\n",
    "- additional hyperparameters increase the effort exponentially\n",
    "- with large step sizes there is the risk to end up in a local maxima/minima and miss the global optimum\n",
    "\n",
    "### What does grid search optimize?\n",
    "(general question)\n",
    "\n",
    "It depends on what the overall algorithm wants to achieve. So a metric must be selected (e.g. Accuracy Score, Precision, f1-Score etc.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde76f6",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Regression\n",
    "\n",
    "### Regularization for regression\n",
    "\n",
    "| are these statements `true` or `false` ? | `true` | `false` |\n",
    "|:------|:--:|:--:|\n",
    "|  LASSO regularization uses the L2 norm of the vector of parameters to be estimated. |  | ‚ùå |\n",
    "|  LASSO regularization can be used for feature selection, because it forces a spase output where some parameters are set to zero.  | ‚úÖ |  |\n",
    "| Regularization for for any learner can be done by adding a penalty to the cost (loss) function that penalizes too complex models. | ‚úÖ |  |\n",
    "| The parameter etimates for ridge regression can be calculated directy using an analytical solution. | ‚úÖ |  |\n",
    "\n",
    "### Which of the following statements about regularization and the regularization parameter Œª are correct ?\n",
    "\n",
    "| are these statements `true` or `false` ? | `true` | `false` |\n",
    "|:------|:--:|:--:|\n",
    "| Using too large a value of Œª can cause your hypothesis to underÔ¨Åt the data. | ‚úÖ |  |\n",
    "|  Using too large a value of Œª can cause your hypothesis to overÔ¨Åt the data.  |  | ‚ùå |\n",
    "| Using a very large value of Œª cannot hurt the performance of your hypothesis. |  | ‚ùå |\n",
    "| None of the above.  |  | ‚ùå |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4690d2c",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "1. Forward Propagation \n",
    "2. Calculate Loss (Error)\n",
    "3. Backward Propagation (Gradient)\n",
    "4. Apply Gradient Descent\n",
    "5. Repeat until convergence\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "We only need the gradient of the Loss function with respect to every parameter\n",
    "\n",
    "Logistic Regression Loss\n",
    "\n",
    "Problems:\n",
    "- local minima\n",
    "\n",
    "\n",
    "Max Likelihood\n",
    "\n",
    "### Deep Learning\n",
    "\n",
    "When **Big Data** is available, **Neural Networks** perform better than classical Machine Learning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc32980d",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eb7771",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Reinforcement Learning \n",
    "\n",
    "\n",
    "### What is the definition of a _finite Markov decision process_ (MDP)?\n",
    "\n",
    "\n",
    "### What is the definition of a _Markov reward process_ (MRP)?\n",
    "\n",
    "\n",
    "### What is Q-learning?\n",
    "\n",
    "\n",
    "### What is Value Iteration?\n",
    "\n",
    "\n",
    "### the diÔ¨Äerence between on-policy and oÔ¨Ä-policy learning.\n",
    "\n",
    "\n",
    "### explain the diÔ¨Äerence between value iteration and policy iteration.\n",
    "\n",
    "\n",
    "### the trade-oÔ¨Ä between exploitation and exploration.\n",
    "\n",
    "\n",
    "### Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9482b303",
   "metadata": {},
   "source": [
    "## Maximum Likelihood\n",
    "\n",
    "\n",
    "### The maximum likelihood model parameters $\\alpha$ can be learned using linear regression for the model: $y_i=\\alpha_1 x_1 x_2^3+\\epsilon_i$ where $\\epsilon_i \\~ N(0,\\sigma^2)$ iid noise.\n",
    "\n",
    "This is true. $y$ is linear in $\\alpha_1$, so it can be learned using linear regression.\n",
    "\n",
    "### The maximum likelihood model parameters $\\alpha$ can be learned using linear regression for the model: $y_i=x_1 ^{\\alpha_1} \\cdot e^{\\alpha_2}+\\epsilon_i$ where $\\epsilon_i \\~ N(0,\\sigma^2)$ iid noise.\n",
    "\n",
    "This is false. $y$ is not linear in $\\alpha_1$ and $\\alpha_2$, and no simple transformation will make it linear\n",
    "($\\log [x^{\\alpha_1} \\cdot e^{\\alpha_2} +\\epsilon_i] \\neq \\alpha_1 \\log x_1 + alpha_2 + \\epsilon_i$).\n",
    "\n",
    "### You are a reviewer for the International Mega-Conference on Algorithms for Radical Learning of Outrageous StuÔ¨Ä, and you read papers with the following experimental setups.\n",
    "\n",
    "> My algorithm is better than yours. Look at the training error rates!\n",
    "\n",
    "Would you accept or reject each paper? Provide a one sentence justiÔ¨Åcation. (This conference has short reviews.)\n",
    "\n",
    "- Reject - the training error is optimistically biased.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844053da",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Probabilistic Reasoning (Bayes)\n",
    "\n",
    "You can name the prior probability distribution, the likelihood function, the \n",
    "evidence, and you know how to marginalize over a joint probability distribution.\n",
    "\n",
    "you know the basic properties of a multivariate Gaussian probability distribution. \n",
    "\n",
    "kernel density estimation method.\n",
    "\n",
    "Probabilities are independent if: $p(A,B) = p(A)\\cdot p(B)$\n",
    "\n",
    "### Bayes Rule & Marginalization\n",
    "\n",
    "$$\n",
    "p(Y|X) = \\frac{p(X|Y)\\cdot p(Y)}{p(X)} = \\frac{p(X|Y)\\cdot p(Y)}{ \\sum_y p(X|y)\\cdot p(y) }\n",
    "$$\n",
    "\n",
    "\n",
    "### What is the difference between a joint and a conditional probability distribution\n",
    "\n",
    "- **Joint** probability is the probability of two events occurring **simultaneously**.\n",
    "- **Marginal** probability is the probability of an event irrespective of the outcome of another variable.\n",
    "- **Conditional** probability is the probability of one event occurring in the presence of a second event.\n",
    "\n",
    "Conditional Probability:\n",
    "- Probability of Outcome $Y$ given Event $X$ --> $p(Y|X)$\n",
    "\n",
    "Joint Probability: --> $p(X,Y)$\n",
    "\n",
    "\n",
    "### What is the Mahalanobis Distance?\n",
    "\n",
    "The Mahalanobis distance is a measure of the **distance between a point P and a distribution D**. It is a **multidimensional** generalization of the idea of measuring **how many standard deviations away P is from the mean of D**. This distance is zero for P at the mean of D and grows as P moves away from the mean along each principal component axis.\n",
    "\n",
    "It is part of the **multivariate Gaussian distribution**.\n",
    "\n",
    "\n",
    "### Explain \"regression toward the mean\"\n",
    "\n",
    "The phenomenon that arises if a sample point of a random variable is extreme (nearly an outlier), a future point is likely to be closer to the mean or average.\n",
    "\n",
    "\n",
    "\n",
    "### Which model learns joint probability?\n",
    "\n",
    "\n",
    "### Which model learns conditional probability?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4139a318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
